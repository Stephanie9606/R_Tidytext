---
title: "STAT 413/613 Homework: Tidy Text"
author: "Your Name"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: no
    toc_depth: 4
    number_sections: yes
    theme: cerulean
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.align  = "center",
                      fig.height = 5, 
                      fig.width  = 6)
```

# Sentiment Analysis

1. Download the following two works from the early 20^th^ century from Project Gutenberg:
- Upton Sinclair: "*The Jungle*" (1906)
- W.E.B. Du Bois: "*The Quest of the Silver Fleece*" (1911)
```{r}
library(tidyverse)
library(tidytext)
library(gutenbergr)

gutenberg_works() %>% 
  filter(str_detect(title, "The Jungle")) # get id 140

gutenberg_works() %>% 
  filter(str_detect(title, "The Quest of the Silver Fleece")) # get id 15265

t_jungle <- gutenberg_download(140)
t_questSF <- gutenberg_download(15265)
```

2. Write a function `to take an argument of a downloaded book tibble and return it in tidy text format.
- The function must add line and chapter numbers as variables
- The function must unnest tokens at the word level
- The function must remove any Project Gutenberg formatting so only the words remain
- The function must remove any stop_words and filter out any `NA`s
- The function must remove any front matter (words before Chapter 1)
- The function can consider the unique nature of the front matter but cannot consider exactly how many chapters are in each book based on looking at the data i.e., no math based on knowing the number of chapters. 
```{r}
data("stop_words")

tidybook <- function(x){
  x %>% 
    mutate(linenumber = row_number(),    # add line and chapter numbers as variables
           text = str_replace(text, "_Contents_", "Contents"),
           text = str_replace(text, "_Note_", "Note"),
           chapter = cumsum(str_detect(text,
                                       regex("(^chapter [ivxlc])|(^_[a-z]+_$)",
                                             ignore_case = TRUE)))) %>% 
    unnest_tokens(word, text) %>%        # unnest tokens at the word level
    mutate(word = str_extract(word, "[a-z']+")) %>% # only the words remain
    anti_join(stop_words, by = "word") %>%         # filter out stop_words and any `NA`s
    drop_na() %>% 
    filter(chapter != 0) ->                        # remove any front matter
    y
  y
}
```

3. Use the function from step 2
- Tidy each book and then add `book` and `author` as variables and save each tibble to a new variable. How many rows are in each book?
```{r}
tidybook(t_jungle) %>% 
  mutate(book = "The Jungle",
         author = "Sinclair, Upton") ->
  t_jungle_tidy
nrow(t_jungle_tidy)

tidybook(t_questSF) %>% 
  mutate(book = "The Quest of the Silver Fleece: A Novel",
         author = "Du Bois, W. E. B. (William Edward Burghardt)") ->
  t_questSF_tidy
nrow(t_questSF_tidy)
```

4. Use a dplyr function to combine the two tibbles into a new tibble. 
- It should have 89,434 rows with 6 variables
```{r}
full_join(t_jungle_tidy, t_questSF_tidy) ->
  twobooks_tidy
nrow(twobooks_tidy)
```

5. Measure the net sentiment using bing for each block of 50 lines
- Plot the sentiment for each book in an appropriate faceted plot - either line or column. 
- Be sure to remove the legend.
- Save the plot to a variable
- Interpret the plots for each book and compare them.
```{r}
twobooks_tidy %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>% 
  mutate(net = positive - negative) %>% 
  ggplot(aes(index, net, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") ->
  books_bing_plot
books_bing_plot
# Interpretation: "The Jungle" seems to be more negative in the middle of the story while "The Quest of the Silver Fleece" has more negative plot in the beginning
```

6. Measure the total for each nrc sentiment in each block of 500 lines and then,
- Filter out the "positive" and "negative" and save to a new variable. You should have 464 observations.
- Plot the count of the sentiments for each block in each book in an appropriate faceted plot with the books in two columns and the sentiments in 8 rows. 
- Be sure to remove the legend.
- Interpret the plots for each book and then compare them. 
- Why did the values drop off so suddenly at the end?
```{r}
twobooks_tidy %>% 
  inner_join(get_sentiments("nrc"), by = "word") %>% 
  count(book, index = linenumber %/% 500, sentiment) %>% 
  filter(!(sentiment == "negative" | sentiment == "positive")) %>% # 464
  # pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n=0)) %>% 
  group_by(book, sentiment, index) %>% 
  count() %>% 
  ungroup() %>% 
  ggplot(aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

7. Using bing, create a new data frame with the counts of the positive and negative sentiment words for each book.
- Show the "top 20" most frequent words across both books along with their book, sentiment, and count, in descending order by count.
- What are the positive words in the list of "top 20"?
```{r}
# new df
twobooks_tidy %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  group_by(book, word, sentiment) %>% 
  count() ->
  books_bing_count
# "top 20" most frequent words across both books
books_bing_count %>% 
  arrange(desc(n)) %>% 
  head(20)
# positive words in the list of "top 20"?
books_bing_count %>% 
  arrange(desc(n)) %>% 
  head(20) %>% 
  filter(sentiment == "positive")
```

8. Plot the top ten for each positive and negative sentiment faceting by book.
- Ensure each facet has the words in the proper order for that book.
- Identify any that may be inappropriate for the context of the book and should be excluded from the sentiment analysis.
```{r}
books_bing_count %>%
  group_by(book, sentiment) %>% 
  slice_max(order_by = n, n = 10) %>% 
  ungroup() %>% 
  mutate(word = reorder_within(word, n, book)) %>%
  ungroup() %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(sentiment ~book, scales = "free") +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
# "miss" may be inappropriate for the context of the book
```

9. Remove the inappropriate word(s) from the analysis.
- Replot the top 10 for each sentiment per book from step 8.
- Interpret the plots
```{r}
# filter out miss
get_sentiments("bing") %>%
  filter(word != "miss") ->
bing_no_miss

# no miss df
twobooks_tidy %>% 
  inner_join(bing_no_miss, by = "word") %>% 
  group_by(book, word, sentiment) %>% 
  count() ->
  books_bing_nomiss

books_bing_nomiss %>% 
  group_by(book, sentiment) %>% 
  slice_max(order_by = n, n = 10) %>% 
  ungroup() %>% 
  mutate(word = reorder_within(word, n, book)) %>%
  ungroup() %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(sentiment ~book, scales = "free") +
  scale_x_reordered() +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()
# Interpretation: "miss" may not be the negative word in the novel, it may refers to young lady, the new plot shows that "slowly" is the most common negative word in "The Quest" with understandable counts, since "miss" is over 400
```

10. Rerun the analysis from step 5 and recreate the plot with the title "Custom Bing".
- Show both the original step 5 plot with the new plot in the same output graphic, one on top of the other.
- Interpret the plots
```{r}
# from step5
twobooks_tidy %>% 
  inner_join(get_sentiments("bing"), by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>% 
  mutate(net = positive - negative) %>% 
  ggplot(aes(index, net, fill = book)) +
  ggtitle("With Miss as Negative" ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") ->
  p1

twobooks_tidy %>% 
  inner_join(bing_no_miss, by = "word") %>% 
  count(book, index = linenumber %/% 50, sentiment) %>% 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = list(n = 0)) %>% 
  mutate(net = positive - negative) %>% 
  ggplot(aes(index, net, fill = book)) +
  ggtitle("Without Miss as Negative" ) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x") ->
  p2

library(gridExtra)
grid.arrange(p1, p2, nrow=2)

# Interpretation: "The Quest" is slightly not that negative after filtering out "miss"
```

# tf-idf for Mark Twain's books

1. Use a single call to download all the following complete books at once from author Mark Twain from Project Gutenberg
- Use the meta_fields argument to include the Book title as part of the download
- *Huckleberry Finn*,  *Tom Sawyer* , *Connecticut Yankee in King Arthur's Court*, *Life on the Mississippi* , *Prince and the Pauper*,  and *A Tramp Abroad* 
```{r}
gutenberg_works() %>% 
  filter(str_detect(author, "Twain")) # get ids: 74, 76, 86, 119, 245, 1837
mtsbooks_df <- gutenberg_download(c(74, 76, 86, 119, 245, 1837), meta_fields = "title") # download the books
```

2. Modify your earlier function or create a new one to output a tf-idf ready dataframe (**leave the the stop words in the text**)
- Unnest, remove any formatting, and get rid of any `NA`s  
- Add the count for each word by title.
- Use your function to tidy the downloaded texts and save to a variable. It should have 56,759 rows.
```{r}
to_tfidf <- function(x){
  x %>% 
    unnest_tokens(word, text) %>%                   # unnest tokens at the word level
    mutate(word = str_extract(word, "[a-z']+")) %>% # only the words remain
    drop_na() %>% 
    count(title, word, sort = TRUE) ->
    y
  
  y %>% 
    group_by(title) %>% 
    summarise(total = sum(n), .groups = "keep") ->
    total_words
  
  y %>% 
    left_join(total_words, by = "title") ->
    y
  y
}
to_tfidf(mtsbooks_df) ->
  mtsbooks_tfidf
```

3. Calculate the tf-idf
- Save back to the data frame.
```{r}
mtsbooks_tfidf %>% 
  bind_tf_idf(word, total, n) ->
  mtsbooks_tfidf
```

4. Plot the tf for each book using a faceted graph.
- Facet by book and constrain the data or the X axis to see the shape of the distribution.
```{r}
mtsbooks_tfidf %>%
  mutate(rank = row_number()) %>% 
  ggplot(aes(rank, tf)) + 
  # geom_abline(intercept = -0.62, slope = -1.1, color = "red", linetype = 2) +
  geom_line(size = 1.1, alpha = 0.8, show.legend = FALSE) + 
  facet_wrap(~title, scales = "free_y") +
  scale_x_log10() +
  scale_y_log10()
```

5. Show the words with the 15 highest tf-idfs across across all books
- Only show those rows.
- How many look like possible names?
```{r}
mtsbooks_tfidf %>% 
  arrange(desc(tf_idf)) %>% 
  slice(1:15)
# it seems like most of them are names except: prince, warn't, didn't, don't, ain, pilots and ain't
```
   
6.  Plot the top 7 tf_idf words from each book.
- Sort in descending order of tf_idf
- Interpret the plots.
```{r}
mtsbooks_tfidf %>% 
  group_by(title) %>% 
  top_n(7, tf_idf) %>% 
  ungroup() %>%
  mutate(word = fct_reorder(word, tf_idf)) %>%
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip()
```

# Extra Credit Podcasts

- Choose **One** of the following podcasts and answer the questions below:  

a. [Sentiment Preserving Fake Reviews](https://podcasts.apple.com/us/podcast/data-skeptic/id890348705?i=1000483067378)  
The [Original paper](https://arxiv.org/abs/1907.09177)

b. [Data in  Life: Authorship Attribution in Lennon-McCartney Songs](https://podcasts.apple.com/us/podcast/authorship-attribution-of-lennon-mccartney-songs/id890348705?i=1000485519404)

1. What are some key ideas from this podcast relevant to text sentiment analysis/authorship attribution?

2. How do you think the ideas discussed may be relevant in your future work?
